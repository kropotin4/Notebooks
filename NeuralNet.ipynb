{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1SifN7FERY0K8FT_BLkJtE7sOMc-DVl95",
      "authorship_tag": "ABX9TyMBZ7GLS7IDWXOeBp3Zi88r",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kropotin4/Notebooks/blob/master/NeuralNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ch3uMDysZpMl"
      },
      "source": [
        "# Нейронная сеть"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTR2V42BXDj2"
      },
      "source": [
        "#!pip install pycuda\n",
        "#!pip install scikit-cuda\n",
        " \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error, log_loss\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from tqdm.notebook import tqdm_notebook\n",
        "import copy\n",
        "import pickle\n",
        "from numba import njit\n",
        " \n",
        "#import pycuda.gpuarray as gpuarray\n",
        "#import skcuda.linalg as linalg\n",
        "#import pycuda.autoinit\n",
        " \n",
        "%matplotlib inline"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5iNP9gvX_TZ"
      },
      "source": [
        "class FFSNNetwork():\n",
        " \n",
        "    def init_WB(self):\n",
        "        \"\"\" Random init Weight and Bias\"\"\"\n",
        "        \n",
        "        for i in range(self.nh + 1):\n",
        "            self.W[i + 1] = np.random.randn(self.sizes[i], self.sizes[i + 1])\\\n",
        "                        * np.sqrt(2 / (self.sizes[i]))  # Correct for min NaN collapse\n",
        "            self.B[i + 1] = np.zeros((1, self.sizes[i + 1]))\n",
        "        \n",
        "        if self.calc_type == 'gpu': # GPUArray\n",
        "            for i in range(self.nh + 1):\n",
        "                self.W[i + 1] = gpuarray.to_gpu(self.W[i + 1])\n",
        "                self.B[i + 1] = gpuarray.to_gpu(self.B[i + 1])  \n",
        " \n",
        "    def get_wb(self):\n",
        "        return {'w': copy.deepcopy(self.W), 'b': copy.deepcopy(self.B)}\n",
        "\n",
        "    def set_wb(self, w, b):\n",
        "        self.W = copy.deepcopy(w)\n",
        "        self.B = copy.deepcopy(b)\n",
        "\n",
        "    def write_weight_bias(self, filepath):\n",
        "        with open(filepath, 'wb') as pfile:\n",
        "            pickle.dump(self.get_wb(), pfile)\n",
        " \n",
        "    def read_weight_bias(self, filepath):\n",
        "        with open(filepath, 'rb') as pfile:\n",
        "            rdata = pickle.load(pfile)\n",
        "            self.W = rdata['w']\n",
        "            self.B = rdata['b']\n",
        " \n",
        "    def __init__(self, n_inputs, n_outputs, hidden_sizes=[2]):\n",
        "        # Init function\n",
        "        # n_inputs - Number of inputs going into the network.\n",
        "        # n_outputs - Number of neurons in last layer: 1 - regression, else - classification\n",
        "        # hidden_sizes - Expects a list of integers, represents the number of neurons present in the hidden layer.\n",
        " \n",
        "        # intialize the inputs\n",
        "        self.nx = n_inputs\n",
        "        self.ny = n_outputs\n",
        "        self.nh = len(hidden_sizes)\n",
        "        self.sizes = [self.nx] + hidden_sizes + [self.ny]\n",
        " \n",
        "        self.calc_type = 'cpu'\n",
        "\n",
        "        self.W = {}\n",
        "        self.B = {}\n",
        "        self.init_WB()\n",
        " \n",
        "    @staticmethod\n",
        "    @njit\n",
        "    def h_cpu(x):\n",
        "        \"\"\" Activation function \"\"\"\n",
        "        #return x / (1.0 + np.exp(-x)) # Swish\n",
        "        return np.where(x > 0, x, 0.01 * x) # Lucky RuLU\n",
        "        #return x * (x > 0) # RuLu\n",
        "        #return 1.0 / (1.0 + np.exp(-x)) # sigmoid\n",
        " \n",
        "    @staticmethod\n",
        "    ##@njit\n",
        "    def h_gpu(x):\n",
        "        \"\"\" Activation function \"\"\"\n",
        "        #return x / (1.0 + np.exp(-x)) # Swish\n",
        "        return gpuarray.maximum(x, 0.01*x)  # Lucky RuLU\n",
        "        #return x * (x > 0) # RuLu\n",
        "        #return 1.0 / (1.0 + np.exp(-x)) # sigmoid\n",
        "\n",
        "    @staticmethod\n",
        "    @njit\n",
        "    def grad_h_cpu(x):\n",
        "        \"\"\" Derivative activation function \"\"\"\n",
        "        #s = x / (1.0 + np.exp(-x))\n",
        "        #return s + (1.0 / (1.0 + np.exp(-x))) * (1 - s)\n",
        "        return np.where(x > 0, 1, 0.01) # Lucky RuLU (require A)\n",
        "        #return 1. * (x > 0) # RuLu\n",
        "        #return x * (1 - x) # sigmoid (require H)\n",
        " \n",
        "    @staticmethod\n",
        "    def grad_h_gpu(x):\n",
        "        \"\"\" Derivative activation function \"\"\"\n",
        "        #s = x / (1.0 + np.exp(-x))\n",
        "        #return s + (1.0 / (1.0 + np.exp(-x))) * (1 - s)\n",
        "        return gpuarray.maximum(x > 0, 0.01) # Lucky RuLU (require A)\n",
        "        #return 1. * (x > 0) # RuLu\n",
        "        #return x * (1 - x) # sigmoid (require H)\n",
        "\n",
        "    @staticmethod\n",
        "    @njit\n",
        "    def softmax(x):\n",
        "        exps = np.exp(x)\n",
        "        return exps / np.sum(exps)\n",
        " \n",
        " \n",
        "    def forward_pass_cpu(self, x):\n",
        "        self.A = {}\n",
        "        self.H = {}\n",
        "        self.A[0] = x.reshape(1, -1)\n",
        "        self.H[0] = x.reshape(1, -1)\n",
        "        for i in range(self.nh):\n",
        "            self.A[i + 1] = np.matmul(self.H[i], self.W[i + 1]) + self.B[i + 1]\n",
        "            self.H[i + 1] = self.h_cpu(self.A[i + 1])\n",
        " \n",
        "        if self.ny == 1: # Regression\n",
        "            self.A[self.nh + 1] = np.matmul(self.H[self.nh], self.W[self.nh + 1]) \\\n",
        "                                 + self.B[self.nh + 1]\n",
        "            self.H[self.nh + 1] = self.A[self.nh + 1] # self.h_cpu(self.A[self.nh + 1])\n",
        "        else: # Classification\n",
        "            self.A[self.nh + 1] = np.matmul(self.H[self.nh], self.W[self.nh + 1]) \\\n",
        "                                 + self.B[self.nh + 1]\n",
        "            self.H[self.nh + 1] = self.softmax(self.A[self.nh + 1])\n",
        " \n",
        "        return self.H[self.nh + 1]\n",
        " \n",
        "    def forward_pass_gpu(self, x):\n",
        "        self.A = {}\n",
        "        self.H = {}\n",
        "        self.A[0] = x.reshape(1, -1)\n",
        "        self.H[0] = x.reshape(1, -1)\n",
        "\n",
        "        for i in range(self.nh):\n",
        "            self.A[i + 1] = linalg.dot(self.H[i], self.W[i + 1]) + self.B[i + 1]\n",
        "            self.H[i + 1] = self.h_gpu(self.A[i + 1])\n",
        " \n",
        "        if self.ny == 1: # Regression\n",
        "            self.A[self.nh + 1] = linalg.dot(self.H[self.nh], self.W[self.nh + 1]) \\\n",
        "                                + self.B[self.nh + 1]\n",
        "            self.H[self.nh + 1] = self.A[self.nh + 1] # self.h_gpu(self.A[self.nh + 1])\n",
        "        else: # Classification\n",
        "            self.A[self.nh + 1] = linalg.dot(self.H[self.nh], self.W[self.nh + 1]) \\\n",
        "                                + self.B[self.nh + 1]\n",
        "            self.H[self.nh + 1] = self.softmax(self.A[self.nh + 1])\n",
        " \n",
        "        \n",
        "        return self.H[self.nh + 1]\n",
        "\n",
        "    def grad_cpu(self, x, y):\n",
        "        self.forward_pass_cpu(x)\n",
        "        self.dW = {}\n",
        "        self.dB = {}\n",
        "        self.dH = {}\n",
        "        self.dA = {}\n",
        "        L = self.nh + 1\n",
        "        self.dA[L] = (self.H[L] - y)\n",
        "        for k in range(L, 1, -1):\n",
        "            self.dW[k] = np.matmul(self.H[k - 1].T, self.dA[k])\n",
        "            self.dB[k] = self.dA[k]\n",
        "            self.dH[k - 1] = np.matmul(self.dA[k], self.W[k].T)\n",
        "            self.dA[k - 1] = np.multiply(self.dH[k - 1], self.grad_h_cpu(self.A[k - 1])) # H[k-1] to A[k-1]\n",
        "        # First layer (dH[0] and dA[0] - useless)\n",
        "        self.dW[1] = np.matmul(self.H[0].T, self.dA[1])\n",
        "        self.dB[1] = self.dA[1]\n",
        " \n",
        "    def grad_gpu(self, x, y):\n",
        "        self.forward_pass_gpu(x)\n",
        "        y = y.reshape(1, -1)\n",
        "        self.dW = {}\n",
        "        self.dB = {}\n",
        "        self.dH = {}\n",
        "        self.dA = {}\n",
        "        L = self.nh + 1\n",
        "        self.dA[L] = (self.H[L] - y)\n",
        "        for k in range(L, 1, -1):\n",
        "            self.dW[k] = linalg.dot(linalg.transpose(self.H[k - 1]), self.dA[k])\n",
        "            self.dB[k] = self.dA[k]\n",
        "            self.dH[k - 1] = linalg.dot(self.dA[k], linalg.transpose(self.W[k]))\n",
        "            self.dA[k - 1] = linalg.multiply(self.dH[k - 1], self.grad_h_gpu(self.A[k - 1])) # H[k-1] to A[k-1]\n",
        "        # First layer (dH[0] and dA[0] - useless)\n",
        "        self.dW[1] = linalg.dot(linalg.transpose(self.H[0]), self.dA[1])\n",
        "        self.dB[1] = self.dA[1]\n",
        "\n",
        "\n",
        "    def fit(self, features, labels, epochs=1, learning_rate=1, initialise=True, display_loss=False,\n",
        "            adv_train=False, adv_epochs=100, score='mse', calc_type='cpu'):\n",
        "        self.calc_type = calc_type\n",
        "\n",
        "        if self.calc_type == 'gpu':\n",
        "            linalg.init()\n",
        "            features = gpuarray.to_gpu(features)\n",
        "            labels = gpuarray.to_gpu(labels)\n",
        "\n",
        "        grad = self.grad_cpu if self.calc_type == 'cpu' else self.grad_gpu\n",
        "\n",
        "        dW_init = {}\n",
        "        dB_init = {}\n",
        "        for i in range(self.nh + 1):\n",
        "            dW_init[i + 1] = np.zeros((self.sizes[i], self.sizes[i + 1]))\n",
        "            dB_init[i + 1] = np.zeros((1, self.sizes[i + 1]))\n",
        "\n",
        "        if self.calc_type == 'gpu':\n",
        "            for i in range(self.nh + 1):\n",
        "                dW_init[i + 1] = gpuarray.to_gpu(dW_init[i + 1])\n",
        "                dB_init[i + 1] = gpuarray.to_gpu(dW_init[i + 1]) \n",
        "\n",
        "        # initialise w, b\n",
        "        if initialise:\n",
        "            self.init_WB()\n",
        " \n",
        "        if display_loss:\n",
        "            X_test = features\n",
        "            Y_test = labels\n",
        "            loss = []\n",
        " \n",
        "        shfk_cur = 1\n",
        "        for e in tqdm_notebook(range(epochs), total=epochs, unit=\"epoch\"):\n",
        "            shfk_cur += 1\n",
        "            if (adv_train and (shfk_cur == adv_epochs or e == 1)):\n",
        "                shfk_cur = 1\n",
        "                features_sh, labels_sh = shuffle(features, labels) # Shuffle data\n",
        "                X, X_test, Y, Y_test = train_test_split( \\\n",
        "                    features_sh, labels_sh, test_size=0.1) # Split 90% data to train\n",
        "            else:\n",
        "                X = features\n",
        "                Y = labels\n",
        " \n",
        "            dW = dW_init.copy()\n",
        "            dB = dB_init.copy()\n",
        "            for x, y in zip(X, Y):\n",
        "                grad(x, y)\n",
        "                for i in range(self.nh + 1):\n",
        "                    dW[i + 1] += self.dW[i + 1]\n",
        "                    dB[i + 1] += self.dB[i + 1]\n",
        " \n",
        "            m = X.shape[1]\n",
        "            for i in range(self.nh + 1):\n",
        "                self.W[i + 1] -= learning_rate * dW[i + 1] / m\n",
        "                self.B[i + 1] -= learning_rate * dB[i + 1] / m\n",
        "\n",
        "            for i in range(self.nh + 1):\n",
        "                if (np.any(np.isnan(self.W[i + 1])) or np.any(np.isnan(self.B[i + 1]))):\n",
        "                    raise Exception(\"NaN vaule in B/W\")\n",
        "\n",
        "            if display_loss:\n",
        "                Y_pred = self.predict(X_test)\n",
        "                if score == 'mse':\n",
        "                    loss.append(mean_squared_error(Y_pred, Y_test))\n",
        "                elif score == 'log':\n",
        "                    loss.append(log_loss(Y_pred, Y_test))\n",
        "                else:\n",
        "                    raise Exception(\"Available score: 'mse' or 'log'\")\n",
        " \n",
        "        if display_loss:\n",
        "            plt.plot(loss)\n",
        "            plt.xlabel('Epochs')\n",
        "            plt.ylabel('Mean Squared Error' if score == 'mse' else 'Cross Entropy Loss')\n",
        "            plt.show()\n",
        " \n",
        "    def predict(self, X):\n",
        "        forward_pass = self.forward_pass_cpu if self.calc_type == 'cpu' else \\\n",
        "                       self.forward_pass_gpu\n",
        "        Y_pred = []\n",
        "        for x in X:\n",
        "            y_pred = forward_pass(x)\n",
        "            Y_pred.append(y_pred)\n",
        "\n",
        "        y_pred = y_pred if self.calc_type == 'cpu' else y_pred.get() \n",
        "        return np.array(Y_pred).squeeze()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-KLHnyXW0Dj"
      },
      "source": [
        "# Тестирование CPU/GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARuQG43fXBwU"
      },
      "source": [
        "%%script false --no-raise-error\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import time\n",
        "\n",
        "# generating 1000 x 1000 matrices\n",
        "np.random.seed(42)\n",
        "\n",
        "x = np.random.randint(0,256, size=(300,300)).astype(np.float64)\n",
        "\n",
        "y = np.random.randint(0,256, size=(300,300)).astype(np.float64)\n",
        "\n",
        "\n",
        "#computing multiplication time on CPU\n",
        "tic = time.time()\n",
        "\n",
        "z = np.matmul(x,y)\n",
        "\n",
        "toc = time.time()\n",
        "\n",
        "time_taken = toc - tic #time in s\n",
        "\n",
        "print(\"Time taken on CPU (in ms) = {}\".format(time_taken*1000))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92gxz7bnXJtI"
      },
      "source": [
        "%%script false --no-raise-error\n",
        "\n",
        "#!pip install pycuda\n",
        "#!pip install scikit-cuda\n",
        "\n",
        "import pycuda.gpuarray as gpuarray\n",
        "import skcuda.linalg as linalg\n",
        "import pycuda.autoinit\n",
        "\n",
        "#computing multiplication time on GPU\n",
        "\n",
        "linalg.init()\n",
        "\n",
        "# storing the arrays on GPU\n",
        "x_gpu = gpuarray.to_gpu(x)\n",
        "\n",
        "y_gpu = gpuarray.to_gpu(y)\n",
        "\n",
        "tic = time.time()\n",
        "\n",
        "#performing the multiplication\n",
        "z_gpu = linalg.dot(x_gpu, y_gpu)\n",
        "\n",
        "toc = time.time()\n",
        "\n",
        "time_taken = toc - tic #time in s\n",
        "\n",
        "print(\"Time taken on a GPU (in ms) = {}\".format(time_taken*1000))"
      ],
      "execution_count": 6,
      "outputs": []
    }
  ]
}